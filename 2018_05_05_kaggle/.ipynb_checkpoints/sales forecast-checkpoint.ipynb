{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment1: Time series prediction \n",
    "\n",
    "Time series forecasting is an important area of machine learning. t is important because there are so many prediction problems that involve a time component. \n",
    "In this notebook we will apply different sequence prediciton techniques to predict the output based on previous days, months or years data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Load Time Series Data\n",
    "\n",
    "This is a dataset that reports on the weather and the level of pollution each hour for five years at the US embassy in Beijing, China.\n",
    "\n",
    "The data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete feature list in the raw data is as follows:\n",
    "\n",
    "1. No: row number\n",
    "2. year: year of data in this row\n",
    "3. month: month of data in this row\n",
    "4. day: day of data in this row\n",
    "5. hour: hour of data in this row\n",
    "6. pm2.5: PM2.5 concentration\n",
    "7. DEWP: Dew Point\n",
    "8. TEMP: Temperature\n",
    "9. PRES: Pressure\n",
    "10. cbwd: Combined wind direction\n",
    "11. Iws: Cumulated wind speed\n",
    "12. Is: Cumulated hours of snow\n",
    "13. Ir: Cumulated hours of rain\n",
    "\n",
    "We can use this data and frame a forecasting problem where, given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour.\n",
    "\n",
    "This dataset can be used to frame other forecasting problems.\n",
    "\n",
    "You can download the dataset from the UCI Machine Learning Repository.\n",
    "\n",
    " [Beijing PM2.5 Data Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "The data is not ready to use. We must prepare it first.\n",
    "\n",
    "Below are the first few rows of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>Sales(In ThousandDollars)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>1755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>1729.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009</td>\n",
       "      <td>5</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>607.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>865.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>2579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>702.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>785.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>3905.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>702.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>3733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4067.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1056.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>757.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>4525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>803.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>WomenClothing</td>\n",
       "      <td>5664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>MenClothing</td>\n",
       "      <td>1070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>OtherClothing</td>\n",
       "      <td>1967.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Month ProductCategory  Sales(In ThousandDollars)\n",
       "0    2009      1   WomenClothing                     1755.0\n",
       "1    2009      1     MenClothing                      524.0\n",
       "2    2009      1   OtherClothing                      936.0\n",
       "3    2009      2   WomenClothing                     1729.0\n",
       "4    2009      2     MenClothing                      496.0\n",
       "5    2009      2   OtherClothing                      859.0\n",
       "6    2009      3   WomenClothing                     2256.0\n",
       "7    2009      3     MenClothing                      542.0\n",
       "8    2009      3   OtherClothing                      921.0\n",
       "9    2009      4   WomenClothing                     2662.0\n",
       "10   2009      4     MenClothing                      669.0\n",
       "11   2009      4   OtherClothing                      914.0\n",
       "12   2009      5   WomenClothing                     2732.0\n",
       "13   2009      5     MenClothing                      650.0\n",
       "14   2009      5   OtherClothing                      989.0\n",
       "15   2009      6   WomenClothing                     2220.0\n",
       "16   2009      6     MenClothing                      607.0\n",
       "17   2009      6   OtherClothing                      932.0\n",
       "18   2009      7   WomenClothing                     2164.0\n",
       "19   2009      7     MenClothing                      575.0\n",
       "20   2009      7   OtherClothing                      901.0\n",
       "21   2009      8   WomenClothing                     2371.0\n",
       "22   2009      8     MenClothing                      551.0\n",
       "23   2009      8   OtherClothing                      865.0\n",
       "24   2009      9   WomenClothing                     2421.0\n",
       "25   2009      9     MenClothing                      579.0\n",
       "26   2009      9   OtherClothing                      819.0\n",
       "27   2009     10   WomenClothing                     2579.0\n",
       "28   2009     10     MenClothing                      610.0\n",
       "29   2009     10   OtherClothing                      914.0\n",
       "..    ...    ...             ...                        ...\n",
       "186  2014      3   WomenClothing                     4268.0\n",
       "187  2014      3     MenClothing                      702.0\n",
       "188  2014      3   OtherClothing                     1293.0\n",
       "189  2014      4   WomenClothing                     4223.0\n",
       "190  2014      4     MenClothing                      785.0\n",
       "191  2014      4   OtherClothing                     1196.0\n",
       "192  2014      5   WomenClothing                     4421.0\n",
       "193  2014      5     MenClothing                      801.0\n",
       "194  2014      5   OtherClothing                     1257.0\n",
       "195  2014      6   WomenClothing                     3905.0\n",
       "196  2014      6     MenClothing                      702.0\n",
       "197  2014      6   OtherClothing                        NaN\n",
       "198  2014      7   WomenClothing                     3733.0\n",
       "199  2014      7     MenClothing                      655.0\n",
       "200  2014      7   OtherClothing                     1175.0\n",
       "201  2014      8   WomenClothing                     4067.0\n",
       "202  2014      8     MenClothing                      692.0\n",
       "203  2014      8   OtherClothing                     1337.0\n",
       "204  2014      9   WomenClothing                     4481.0\n",
       "205  2014      9     MenClothing                        NaN\n",
       "206  2014      9   OtherClothing                     1056.0\n",
       "207  2014     10   WomenClothing                     4434.0\n",
       "208  2014     10     MenClothing                      757.0\n",
       "209  2014     10   OtherClothing                     1153.0\n",
       "210  2014     11   WomenClothing                     4525.0\n",
       "211  2014     11     MenClothing                      803.0\n",
       "212  2014     11   OtherClothing                     1468.0\n",
       "213  2014     12   WomenClothing                     5664.0\n",
       "214  2014     12     MenClothing                     1070.0\n",
       "215  2014     12   OtherClothing                     1967.0\n",
       "\n",
       "[216 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "dataset = read_csv('Train_Kaggle.csv')\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first step is to consolidate the date-time information into a single date-time so that we can use it as an index in Pandas.\n",
    "\n",
    "- A quick check reveals NA values for pm2.5 for the first 24 hours. We will, therefore, need to remove the first row of data. There are also a few scattered “NA” values later in the dataset; we can mark them with 0 values for now.\n",
    "\n",
    "- The script below loads the raw dataset and parses the date-time information as the Pandas DataFrame index. The “No” column is dropped and then clearer names are specified for each column. Finally, the NA values are replaced with “0” values and the first 24 hours are removed.\n",
    "\n",
    "- The “No” column is dropped and then clearer names are specified for each column. Finally, the NA values are replaced with “0” values and the first 24 hours are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     pollution  dew  temp   press wnd_dir  wnd_spd  snow  rain\n",
      "date                                                                          \n",
      "2010-01-02 00:00:00      129.0  -16  -4.0  1020.0      SE     1.79     0     0\n",
      "2010-01-02 01:00:00      148.0  -15  -4.0  1020.0      SE     2.68     0     0\n",
      "2010-01-02 02:00:00      159.0  -11  -5.0  1021.0      SE     3.57     0     0\n",
      "2010-01-02 03:00:00      181.0   -7  -5.0  1022.0      SE     5.36     1     0\n",
      "2010-01-02 04:00:00      138.0   -7  -5.0  1022.0      SE     6.25     2     0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# load data\n",
    "\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')\n",
    "\n",
    "dataset = read_csv('Train_Kaggle.csv', \n",
    "                   parse_dates = [['year', 'month']], index_col=0, date_parser=parse)\n",
    "\n",
    "dataset.drop('No', axis=1, inplace=True)\n",
    "\n",
    "# manually specify column names\n",
    "dataset.columns = ['prod_category', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "dataset.index.name = 'date'\n",
    "\n",
    "# mark all NA values with 0\n",
    "dataset['pollution'].fillna(0, inplace=True)\n",
    "\n",
    "# drop the first 24 hours\n",
    "dataset = dataset[24:]\n",
    "# summarize first 5 rows\n",
    "print(dataset.head(5))\n",
    "# save to file\n",
    "dataset.to_csv('pollution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "we can create a quick plot of each series and see what we have.\n",
    "\n",
    "The code below loads the new “pollution.csv” file and plots each series as a separate subplot, except wind speed dir, which is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85e99b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "\n",
    "# specify columns to plot , skip 4 becoz it is categorical\n",
    "groups = [0, 1, 2, 3, 5, 6, 7]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize=(20,15))\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Data for Time Series Prediction\n",
    "\n",
    "- The first step is to prepare the pollution dataset for the Time Series problem.\n",
    "\n",
    "- This involves framing the dataset as a supervised learning problem and normalizing the input variables.\n",
    "\n",
    "- We will frame the supervised learning problem as predicting the pollution at the current hour (t) given the pollution measurement and weather conditions at the prior time step. So, the input is features at time step(t-1) and output is pollution at time t. \n",
    "\n",
    "- We can transform the dataset using the series_to_supervised() function given in code\n",
    "\n",
    "\n",
    "\n",
    "- Steps to convert the  Time Series to a Supervised Learning Problem are:\n",
    "    \n",
    "    1. Load the “pollution.csv” dataset \n",
    "    2. Label encoded (integer encoded) the wind speed feature\n",
    "    3. Normalize the Dataset\n",
    "    4. Transform the dataset into supervised learning problem by calling tghe function \"series_to_supervised\"\n",
    "    \n",
    "    \n",
    "    \n",
    "- Series to supervised function:\n",
    "\n",
    "    - Intution is in time series forecasting terminology the current time (t) and future times (t+1, t+n) are forecast times and past observations (t-1, t-n) are used to make forecasts.\n",
    "    - We can see how positive and negative shifts can be used to create a new DataFrame from a time series with sequences of input and output patterns for a supervised learning problem.\n",
    "    - This permits not only classical X -> y prediction, but also X -> Y where both input and output can be sequences.\n",
    "    - The function takes four arguments:\n",
    "\n",
    "        1. data: Sequence of observations as a list or 2D NumPy array. Required.\n",
    "        2. n_in: Number of lag observations as input (X). Values may be between [1..len(data)] Optional. Defaults to 1.\n",
    "        3. n_out: Number of observations as output (y). Values may be between [0..len(data)-1]. Optional. Defaults to 1.\n",
    "        4 . dropnan: Boolean whether or not to drop rows with NaN values. Optional. Defaults to True.\n",
    "\n",
    "        5. The function returns Pandas DataFrame of series framed for supervised learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_vars = data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43800, 8)\n",
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
      "1   0.129779   0.352941   0.245902   0.527273   0.666667   0.002290   \n",
      "2   0.148893   0.367647   0.245902   0.527273   0.666667   0.003811   \n",
      "3   0.159960   0.426471   0.229508   0.545454   0.666667   0.005332   \n",
      "4   0.182093   0.485294   0.229508   0.563637   0.666667   0.008391   \n",
      "5   0.138833   0.485294   0.229508   0.563637   0.666667   0.009912   \n",
      "\n",
      "   var7(t-1)  var8(t-1)   var1(t)   var2(t)   var3(t)   var4(t)   var5(t)  \\\n",
      "1   0.000000        0.0  0.148893  0.367647  0.245902  0.527273  0.666667   \n",
      "2   0.000000        0.0  0.159960  0.426471  0.229508  0.545454  0.666667   \n",
      "3   0.000000        0.0  0.182093  0.485294  0.229508  0.563637  0.666667   \n",
      "4   0.037037        0.0  0.138833  0.485294  0.229508  0.563637  0.666667   \n",
      "5   0.074074        0.0  0.109658  0.485294  0.213115  0.563637  0.666667   \n",
      "\n",
      "    var6(t)   var7(t)  var8(t)  \n",
      "1  0.003811  0.000000      0.0  \n",
      "2  0.005332  0.000000      0.0  \n",
      "3  0.008391  0.037037      0.0  \n",
      "4  0.009912  0.074074      0.0  \n",
      "5  0.011433  0.111111      0.0  \n",
      "\n",
      "\n",
      "   var1(t-3)  var2(t-3)  var3(t-3)  var4(t-3)  var5(t-3)  var6(t-3)  \\\n",
      "3   0.129779   0.352941   0.245902   0.527273   0.666667   0.002290   \n",
      "4   0.148893   0.367647   0.245902   0.527273   0.666667   0.003811   \n",
      "5   0.159960   0.426471   0.229508   0.545454   0.666667   0.005332   \n",
      "6   0.182093   0.485294   0.229508   0.563637   0.666667   0.008391   \n",
      "7   0.138833   0.485294   0.229508   0.563637   0.666667   0.009912   \n",
      "\n",
      "   var7(t-3)  var8(t-3)  var1(t-2)  var2(t-2)   ...     var7(t-1)  var8(t-1)  \\\n",
      "3   0.000000        0.0   0.148893   0.367647   ...      0.000000        0.0   \n",
      "4   0.000000        0.0   0.159960   0.426471   ...      0.037037        0.0   \n",
      "5   0.000000        0.0   0.182093   0.485294   ...      0.074074        0.0   \n",
      "6   0.037037        0.0   0.138833   0.485294   ...      0.111111        0.0   \n",
      "7   0.074074        0.0   0.109658   0.485294   ...      0.148148        0.0   \n",
      "\n",
      "    var1(t)   var2(t)   var3(t)   var4(t)   var5(t)   var6(t)   var7(t)  \\\n",
      "3  0.182093  0.485294  0.229508  0.563637  0.666667  0.008391  0.037037   \n",
      "4  0.138833  0.485294  0.229508  0.563637  0.666667  0.009912  0.074074   \n",
      "5  0.109658  0.485294  0.213115  0.563637  0.666667  0.011433  0.111111   \n",
      "6  0.105634  0.485294  0.213115  0.581818  0.666667  0.014492  0.148148   \n",
      "7  0.124748  0.485294  0.229508  0.600000  0.666667  0.017551  0.000000   \n",
      "\n",
      "   var8(t)  \n",
      "3      0.0  \n",
      "4      0.0  \n",
      "5      0.0  \n",
      "6      0.0  \n",
      "7      0.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# convert series to supervised learning\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "print(values.shape)\n",
    "\n",
    "# integer encode direction\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "# previous 1 hour features used for framing as supervised learning problem \n",
    "## specify the number of lag hours\n",
    "n_hours1 = 1\n",
    "n_features1 = 8\n",
    "reframed1 = series_to_supervised(scaled, n_hours1, 1)\n",
    "print(reframed1.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# previous 3 hour features used for framing as supervised learning problem \n",
    "## specify the number of lag hours\n",
    "n_hours2 = 3\n",
    "n_features2 = 8\n",
    "reframed2 = series_to_supervised(scaled, n_hours2, 1)\n",
    "print(reframed2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Splitt the Dataset into training and testing data\n",
    "\n",
    "### Case1: Prediction using prev 1 hour data:\n",
    " We will split data into prev 1 year data for training and 4 year of data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35040, 1, 8) (35040,) (8759, 1, 8) (8759,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "values1 = reframed1.values\n",
    "n_train_hours1 = 4*365 * 24\n",
    "n_obs1 = n_hours1 * n_features1\n",
    "\n",
    "train1 = values1[:n_train_hours1, :]\n",
    "test1 = values1[n_train_hours1:, :]\n",
    "\n",
    "# split into input and outputs by taking last column \"pollution\" as target\n",
    "train_X1, train_y1 = train1[:, :n_obs1], train1[:, -n_features1]\n",
    "test_X1, test_y1 = test1[:, :n_obs1], test1[:, -n_features1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features] because LSTM takes 3d data as input\n",
    "train_X1 = train_X1.reshape((train_X1.shape[0], 1, train_X1.shape[1]))\n",
    "test_X1 = test_X1.reshape((test_X1.shape[0], 1, test_X1.shape[1]))\n",
    "print(train_X1.shape, train_y1.shape, test_X1.shape, test_y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case2: Prediction using prev 3 hour data:\n",
    " We will split data into prev 1 year data for training and 4 year of data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "(35040, 3, 8) (35040,) (8757, 3, 8) (8757,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "values2 = reframed2.values\n",
    "n_train_hours2 = 4 * 365 * 24\n",
    "n_obs2 = n_hours2 * n_features2\n",
    "print(n_obs2)\n",
    "train2 = values2[:n_train_hours2, :]\n",
    "test2 = values2[n_train_hours2:, :]\n",
    "\n",
    "# split into input and outputs by taking last column \"pollution\" as target\n",
    "train_X2, train_y2 = train2[:, :n_obs2], train2[:, -n_features2]\n",
    "test_X2, test_y2 = test2[:, :n_obs2], test2[:, -n_features2]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features] because LSTM takes 3d data as input\n",
    "train_X2 = train_X2.reshape((train_X2.shape[0], 3, train_X2.shape[1]//3))\n",
    "test_X2 = test_X2.reshape((test_X2.shape[0], 3, test_X2.shape[1]//3))\n",
    "print(train_X2.shape, train_y2.shape, test_X2.shape, test_y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Prediction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Linear Regression on Lagged Dataset\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rmse(ypred, X_regr, y_regr):\n",
    "    # invert scaling for forecast\n",
    "    inv_ypred = np.concatenate((ypred, X_regr[:, 1:8]), axis=1)\n",
    "    inv_ypred = scaler.inverse_transform(inv_ypred)\n",
    "    inv_ypred = inv_ypred[:,0]\n",
    "\n",
    "    # invert scaling for actual\n",
    "    y_regr = y_regr.reshape((len(y_regr), 1))\n",
    "    inv_y = np.concatenate((y_regr, X_regr[:, 1:8]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_ypred))\n",
    "    print('RMSE: %.3f' % rmse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35040, 8) (35040,) (8759, 8) (8759,)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 2D [samples, timesteps, features] because LR takes 2d data as input\n",
    "train_X1_regr = train_X1.reshape((train_X1.shape[0],train_X1.shape[2]))\n",
    "test_X1_regr = test_X1.reshape((test_X1.shape[0], test_X1.shape[2]))\n",
    "\n",
    "train_y1_regr = train_y1\n",
    "test_y1_regr = test_y1\n",
    "print(train_X1_regr.shape, train_y1_regr.shape, test_X1_regr.shape, test_y1_regr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02350276]\n",
      " [0.05087818]\n",
      " [0.06245838]]\n",
      "Train\n",
      "RMSE: 27.744\n",
      "Test\n",
      "RMSE: 24.200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_X1_regr, train_y1_regr)\n",
    "\n",
    "# make a prediction\n",
    "train_y1_pred = regr.predict(train_X1_regr).reshape(train_X1_regr.shape[0],1)\n",
    "test_y1_pred = regr.predict(test_X1_regr).reshape(test_X1_regr.shape[0],1)\n",
    "print(test_y1_pred[:3])\n",
    "\n",
    "print(\"Train\")\n",
    "train_1_rmse = calc_rmse(train_y1_pred, train_X1_regr, train_y1_regr)\n",
    "print(\"Test\")\n",
    "test_1_rmse = calc_rmse(test_y1_pred, test_X1_regr, test_y1_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35040, 24) (35040,) (8757, 24) (8757,)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 2D [samples, timesteps, features] because LR takes 2D data as input\n",
    "train_X2_regr = train_X2.reshape((train_X2.shape[0], train_X2.shape[1]*train_X2.shape[2]))\n",
    "test_X2_regr = test_X2.reshape((test_X2.shape[0], train_X2.shape[1]*test_X2.shape[2]))\n",
    "\n",
    "train_y2_regr = train_y2\n",
    "test_y2_regr = test_y2\n",
    "print(train_X2_regr.shape, train_y2_regr.shape, test_X2_regr.shape, test_y2_regr.shape)\n",
    "# print(train_X2.shape, train_y2.shape, test_X2.shape, test_y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06383923]\n",
      " [0.06811477]\n",
      " [0.08591244]]\n",
      "Train\n",
      "RMSE: 27.476\n",
      "Test\n",
      "RMSE: 23.955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_X2_regr, train_y2_regr)\n",
    "\n",
    "# make a prediction\n",
    "train_ypred = regr.predict(train_X2_regr).reshape(train_X2_regr.shape[0],1)\n",
    "test_ypred = regr.predict(test_X2_regr).reshape(test_X2_regr.shape[0],1)\n",
    "print(test_ypred[:3])\n",
    "\n",
    "print(\"Train\")\n",
    "train_2_rmse = calc_rmse(train_ypred, train_X2_regr, train_y2_regr)\n",
    "print(\"Test\")\n",
    "test_2_rmse = calc_rmse(test_ypred, test_X2_regr, test_y2_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)   Train and evaluate Deep sequence prediction models RNN/LSTM \n",
    "\n",
    "\n",
    "### Train Model\n",
    "  - We will define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution. The input shape will be 1 time step with 8 features.\n",
    "\n",
    "  - We will use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.\n",
    "\n",
    "  - The model will be fit for 50 training epochs with a batch size of 72. Remember that the internal state of the LSTM in Keras is reset at the end of each batch, so an internal state that is a function of a number of days may be helpful (try testing this).\n",
    "\n",
    "  - Finally, we keep track of both the training and test loss during training by setting the validation_data argument in the fit() function. At the end of the run both the training and test loss are plotted.\n",
    "  \n",
    "  \n",
    "### Evaluate Model\n",
    "\n",
    "- After the model is fit, we can forecast for the entire test dataset.\n",
    "\n",
    "- We combine the forecast with the test dataset and invert the scaling. We also invert scaling on the test dataset with the expected pollution numbers.\n",
    "\n",
    "- With forecasts and actual values in their original scale, we can then calculate an error score for the model. In this case, we calculate the Root Mean Squared Error (RMSE) that gives error in the same units as the variable itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabira\\conda64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_and_evaluate_lstm(train_X, train_y, test_X, test_y):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(train_X.shape[1], train_X.shape[2],)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=10, batch_size=64, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # make a prediction\n",
    "    train_ypred = model.predict(train_X)\n",
    "    test_ypred = model.predict(test_X)\n",
    "    print(test_ypred[:3])\n",
    "    \n",
    "    train_X = train_X.reshape((train_X.shape[0], train_X.shape[1]*train_X.shape[2]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[1]*test_X.shape[2]))\n",
    "    \n",
    "    train_rmse = calc_rmse(train_ypred, train_X, train_y)\n",
    "    test_rmse = calc_rmse(test_ypred, test_X, test_y)\n",
    "    return train_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case1: Train and evaluate the model for sequence prediction using previous 1 hour data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35040 samples, validate on 8759 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.0041 - val_loss: 0.0049\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 3/10\n",
      " - 2s - loss: 8.0804e-04 - val_loss: 6.7793e-04\n",
      "Epoch 4/10\n",
      " - 2s - loss: 7.9867e-04 - val_loss: 6.4183e-04\n",
      "Epoch 5/10\n",
      " - 2s - loss: 7.9662e-04 - val_loss: 6.3302e-04\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.9518e-04 - val_loss: 6.3014e-04\n",
      "Epoch 7/10\n",
      " - 2s - loss: 7.9390e-04 - val_loss: 6.2926e-04\n",
      "Epoch 8/10\n",
      " - 2s - loss: 7.9273e-04 - val_loss: 6.2922e-04\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.9165e-04 - val_loss: 6.2952e-04\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.9065e-04 - val_loss: 6.2996e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1086ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0244738 ]\n",
      " [0.05015232]\n",
      " [0.06101431]]\n",
      "RMSE: 28.195\n",
      "RMSE: 24.948\n"
     ]
    }
   ],
   "source": [
    "train_rmse_lstm1, test_rmse_lstm1 = train_and_evaluate_lstm(train_X1, train_y1, test_X1, test_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case2: Train and evaluate the model for sequence prediction using previous 3 hour data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35040 samples, validate on 8757 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.0038 - val_loss: 0.0031\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0010 - val_loss: 8.5912e-04\n",
      "Epoch 4/10\n",
      " - 2s - loss: 9.5069e-04 - val_loss: 7.3485e-04\n",
      "Epoch 5/10\n",
      " - 2s - loss: 8.7106e-04 - val_loss: 6.6022e-04\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.2383e-04 - val_loss: 6.3547e-04\n",
      "Epoch 7/10\n",
      " - 3s - loss: 8.0400e-04 - val_loss: 6.3772e-04\n",
      "Epoch 8/10\n",
      " - 3s - loss: 7.9415e-04 - val_loss: 6.4820e-04\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.8829e-04 - val_loss: 6.6349e-04\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.8446e-04 - val_loss: 6.7313e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e77400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06230807]\n",
      " [0.06659134]\n",
      " [0.0872089 ]]\n",
      "RMSE: 28.618\n",
      "RMSE: 25.789\n"
     ]
    }
   ],
   "source": [
    "train_rmse_lstm3, test_rmse_lstm3 = train_and_evaluate_lstm(train_X2, train_y2, test_X2, test_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "1. Change the window size that is number of hours you want to use for prediction and  see the change in results\n",
    "\n",
    "2. Use any other non-linear regression method to get better resultsm, and tabulate\n",
    "\n",
    "3. Use a sequence prediction method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
